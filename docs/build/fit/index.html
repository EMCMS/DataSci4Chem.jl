<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Curve fitting · DataSci4Chem.jl</title><meta name="title" content="Curve fitting · DataSci4Chem.jl"/><meta property="og:title" content="Curve fitting · DataSci4Chem.jl"/><meta property="twitter:title" content="Curve fitting · DataSci4Chem.jl"/><meta name="description" content="Documentation for DataSci4Chem.jl."/><meta property="og:description" content="Documentation for DataSci4Chem.jl."/><meta property="twitter:description" content="Documentation for DataSci4Chem.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="DataSci4Chem.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">DataSci4Chem.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../Basics/">Basics</a></li><li><a class="tocitem" href="../vis/">Data visualization</a></li><li><a class="tocitem" href="../Matrix/">Matrix manipulation</a></li><li><a class="tocitem" href="../svd/">SVD</a></li><li><a class="tocitem" href="../DataExplore/">Data exploration</a></li><li><a class="tocitem" href="../RootFinding/">Numerical root finding</a></li><li><a class="tocitem" href="../Integration/">Numerical integration</a></li><li class="is-active"><a class="tocitem" href>Curve fitting</a><ul class="internal"><li><a class="tocitem" href="#Least-squares"><span>Least squares</span></a></li><li><a class="tocitem" href="#Nonlinear-fit"><span>Nonlinear fit</span></a></li><li><a class="tocitem" href="#Goodness-of-the-fit"><span>Goodness of the fit</span></a></li><li><a class="tocitem" href="#Additional-resources"><span>Additional resources</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Curve fitting</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Curve fitting</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/EMCMS/DataSci4Chem.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/EMCMS/DataSci4Chem.jl/blob/main/docs/src/fit.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Curve-fitting"><a class="docs-heading-anchor" href="#Curve-fitting">Curve fitting</a><a id="Curve-fitting-1"></a><a class="docs-heading-anchor-permalink" href="#Curve-fitting" title="Permalink"></a></h1><p><a href="https://en.wikipedia.org/wiki/Curve_fitting">Curve fitting</a> is the process of postulating a function that explains the trend in the data only based on how well the variance in the data is explained. Curve fitting may refer to both <a href="https://en.wikipedia.org/wiki/Interpolation">data interpolation</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a>. Here we will mainly focus on the regression. </p><p>There are several algorithms that are used for solving both linear and non-linear regression problems. One of the most commonly used approaches for regression analysis is the <a href="https://en.wikipedia.org/wiki/Least_squares#Differences_between_linear_and_nonlinear_least_squares">least squares</a>.</p><h2 id="Least-squares"><a class="docs-heading-anchor" href="#Least-squares">Least squares</a><a id="Least-squares-1"></a><a class="docs-heading-anchor-permalink" href="#Least-squares" title="Permalink"></a></h2><p>For linear regression problems the least squares solution tries to minimize the squared distances between each measured data point and the potential line/function at the same time. Let&#39;s imagine a case where we have measured the signal associated with different concentrations of a chemical in solution (i.e. <a href="https://en.wikipedia.org/wiki/Calibration_curve">external standard calibration curve</a>). </p><pre><code class="language-julia hljs">using DataSci4Chem

cal = [5.871	1
	   16.224	2
	   16.628	2
	   21.9	    3
	   32.172	4
	   33.006	4
	   44.512	5
	   53.285	6
 	   55.985	6
	   104.403	9]

scatter(cal[:,1],cal[:,2],label = &quot;measured data&quot;,xlabel = &quot;concentration&quot;,ylabel=&quot;signal&quot;)</code></pre><img src="314694d5.svg" alt="Example block output"/><p>The solution to this problem is a function <em><span>$f(x) = ax + b$</span></em>, where <em>a</em> and <em>b</em> are the slope and intercept of the potential line describing these data. As it was stated before, we are aiming at minimizing the squared distances between the <em>f(x)</em> and the data points. The first step will be to define the distance between <em><span>$y_i$</span></em> and <em><span>$f(x_i)$</span></em>, defined as the difference between the two points resulting in the following:</p><p class="math-container">\[
\sum_{i=1}^{n} (d_i)^2 \\
\sum_{i=1}^{n} (y_i - f(x_i))^2
\]</p><p>The least squares algorithm aims at minimizing the sum of these distances (i.e. sum squared residuals) to find the best fitting curve into the data. </p><p class="math-container">\[
\sum_{i=1}^{n} (y_i - f(x_i))^2 \rightarrow 0
\]</p><p>Let&#39;s assume that our data can be described by <em><span>$f(x) = ax + b$</span></em>, which is a simple linear solution to our problem. This implies that we can rewrite the above equation by plugging in the value of <em>f(x)</em> into our equation. </p><p class="math-container">\[
\sum_{i=1}^{n} (y_i - (ax_i + b))^2 \rightarrow 0
\]</p><p>There are different ways of solving this problem namely: analytically and numerically. </p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The analytical solutions are only possible for linear problems while for nonlinear system the solution must be found numerically. </p></div></div><h3 id="Ordinary-least-squares"><a class="docs-heading-anchor" href="#Ordinary-least-squares">Ordinary least squares</a><a id="Ordinary-least-squares-1"></a><a class="docs-heading-anchor-permalink" href="#Ordinary-least-squares" title="Permalink"></a></h3><p>To find the solution to this we could utilize the systems of equations built using the objective function <em>f(x)</em> and the data. For example, if selecting four points in our data, we will end up with the below system. </p><p class="math-container">\[
y_1 = ax_1 + b\\
y_2 = ax_2 + b\\
y_3 = ax_3 + b\\
y_4 = ax_4 + b
\]</p><p>This system of equations can be translated into a set of matrices.</p><p class="math-container">\[
\begin{bmatrix} y_1\\ y_2\\ y_3\\ y_4 \end{bmatrix} = \begin{bmatrix} 1 &amp; x_1\\ 1 &amp; x_2\\ 1 &amp; x_3\\ 1 &amp; x_4 \end{bmatrix} \times 

\begin{bmatrix} b \\ a \end{bmatrix}
\]</p><p>As we saw before we can solve this system using the elimination approach. We also can use the solution of ordinary least squares problem to find the two unknown parameters. </p><p class="math-container">\[
\begin{bmatrix} b \\ a \end{bmatrix} = (X^{T}X)^{-1}X^{T}Y 
\]</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Please note the moment the objective function does not meet the linearity criteria this solution will not work. </p></div></div><pre><code class="language-julia hljs">using DataSci4Chem

X = cal[:,1]
Y = cal[:,2]

X1 = hcat(ones(length(X),1),X)

beta = pinv(X1&#39;*X1)*X1&#39; * Y</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.022936488854152
 0.08273904546379036</code></pre><pre><code class="language-julia hljs">using DataSci4Chem

f(x) = beta[1] .+ beta[2].*x

plot!(X,f(X),label=&quot;model&quot;)</code></pre><img src="502acaed.svg" alt="Example block output"/><h3 id="Numerical-approach"><a class="docs-heading-anchor" href="#Numerical-approach">Numerical approach</a><a id="Numerical-approach-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-approach" title="Permalink"></a></h3><p>This also can be solved numerically. We still start with an objective function <em>f(x)</em> and try to find the parameters of such function through an iterative manner. Let&#39;s start with a randomly selected set of parameters for example <em>a=0</em> and <em>b=10</em>. </p><pre><code class="language-julia hljs">using DataSci4Chem

a_p = 0
b_p = 10

fh(x) = b_p .+ (a_p .* x)

plot!(X,fh(X),label=&quot;random parameters&quot;)</code></pre><img src="5fe31ff9.svg" alt="Example block output"/><p>Now we can calculate the <em>SSR</em> as a metric for how well our random parameters are performing in modeling the trend in our data. For this we first calculate the residuals of the potential model, and then these residuals are squared and summed. </p><pre><code class="language-julia hljs">using DataSci4Chem

SSR = sum((fh(X) .- Y).^2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">388.0</code></pre><p>Now we can update the parameters with new values and repeat this process. </p><pre><code class="language-julia hljs">using DataSci4Chem

a_p = collect(range(-1,2,length=10))
b_p = collect(range(-5,5,length=10))

SSR_m = zeros(length(a_p))

for i=1:length(a_p)

	fh(x) = b_p[i] .+ (a_p[i] .* x)
	SSR_m[i] = sum((fh(X) .- Y).^2)

end

scatter(SSR_m,label=false, xlabel=&quot;try&quot;,ylabel=&quot;SSR&quot;)</code></pre><img src="66cd61a1.svg" alt="Example block output"/><p>Where the <em>SSR</em> plot has a minimum is the set of model parameters with lowest error and thus the best fit. </p><pre><code class="language-julia hljs">using DataSci4Chem

fh(x) = b_p[argmin(SSR_m)] .+ (a_p[argmin(SSR_m)] .* x)

scatter(cal[:,1],cal[:,2],label = &quot;measured data&quot;,xlabel = &quot;concentration&quot;,ylabel=&quot;signal&quot;)
plot!(X,f(X),label=&quot;analytical model&quot;)

plot!(X,fh(X),label=&quot;numerical model 10 steps&quot;,ls =:dot,lw=3)</code></pre><img src="3216550f.svg" alt="Example block output"/><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Higher is the number of the steps, thus higher resolution, of the parameter space exploration, the more accurate results can be obtained. </p></div></div><pre><code class="language-julia hljs">using DataSci4Chem

a_p = collect(range(-1,2,length=1000))
b_p = collect(range(-5,5,length=1000))

SSR_m = zeros(length(a_p))

for i=1:length(a_p)

	fh(x) = b_p[i] .+ (a_p[i] .* x)
	SSR_m[i] = sum((fh(X) .- Y).^2)

end

fh(x) = b_p[argmin(SSR_m)] .+ (a_p[argmin(SSR_m)] .* x)

scatter(cal[:,1],cal[:,2],label = &quot;measured data&quot;,xlabel = &quot;concentration&quot;,ylabel=&quot;signal&quot;)
plot!(X,f(X),label=&quot;analytical model&quot;)

plot!(X,fh(X),label=&quot;numerical model 1000 steps&quot;,ls =:dot,lw=3)</code></pre><img src="9882b83b.svg" alt="Example block output"/><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>You can also use a more data deriven approach to exploring the model parameter space, for example <a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Bayesian regression</a>. More details on how to do this is provided <a href="https://emcms.github.io/ACS.jl/dev/Bayes/">here</a>. </p></div></div><h2 id="Nonlinear-fit"><a class="docs-heading-anchor" href="#Nonlinear-fit">Nonlinear fit</a><a id="Nonlinear-fit-1"></a><a class="docs-heading-anchor-permalink" href="#Nonlinear-fit" title="Permalink"></a></h2><p>There are several numerical approaches that are able to solve both linear and nonlinear problems, referred to <a href="https://en.wikipedia.org/wiki/Non-linear_least_squares">nonlinear solvers</a>. For these solvers to work, they need a starting point and the interest window for each parameter. Depending on the implementation, they may have different kernels to solve such systems. For example the package <a href="https://julianlsolvers.github.io/LsqFit.jl/latest/"><em>LsqFit.jl</em></a> incorporated into <em>DataSci4Chem.jl</em> employs the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm">Gauss-Newton</a> algorithm. </p><p>To solve linear problems you need to define your objective model as a combination of independent variables and the model parameters. For example, our current system is defined as following. </p><pre><code class="language-julia hljs">using DataSci4Chem

model(x,p) = p[1] .* x .+ p[2]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">model (generic function with 1 method)</code></pre><p>Now that the objective function is already built, we can use our data to fit our data. For that we need to provide the algorithm with a set of starting points as well as the data. </p><pre><code class="language-julia hljs">using DataSci4Chem

p0 = [0.5, 0.5]

fit = curve_fit(model, X, Y, p0)
fit.param</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.08273904546404943
 1.022936488844917</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The starting points must be close to the true value of the parameters otherwise, the algorithm may not be able to find the best fit, particularly for more complex systems. </p></div></div><p>For more complex systems, we must provide our algorithm with parameter bounds besides the starting points. These will be particularly helpful for cases that multiple solutions to the system is possible (e.g. Gaussian fit). </p><pre><code class="language-julia hljs">using DataSci4Chem

lb = [-2.0, -2.0]
ub = [2.0, 2.0]

fit = curve_fit(model, X, Y, p0, lower=lb, upper=ub)
fit.param</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.08273904546400941
 1.0229364888464791</code></pre><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>The parameter bounds must be float64! If integers are provided, the algorithm will give you an error.</p></div></div><h2 id="Goodness-of-the-fit"><a class="docs-heading-anchor" href="#Goodness-of-the-fit">Goodness of the fit</a><a id="Goodness-of-the-fit-1"></a><a class="docs-heading-anchor-permalink" href="#Goodness-of-the-fit" title="Permalink"></a></h2><p>Now that we have managed to fit an objective function to our model, we need to evaluate whether the fitted model is good enough, or representative of our data. There are different metrics for this assessment. Here we are going to discuss the residuals, root mean squared error (RMSE), and  coefficient of determination (i.e. <em><span>$R^2$</span></em>).</p><h3 id="Residuals"><a class="docs-heading-anchor" href="#Residuals">Residuals</a><a id="Residuals-1"></a><a class="docs-heading-anchor-permalink" href="#Residuals" title="Permalink"></a></h3><p>Residuals are the difference between each <em><span>$y_i$</span></em> and <em><span>$f(x_i)$</span></em>. For a model that is able to represent our data well without any systematic error, we want to see a random distribution of the residuals across <em>X</em>. </p><pre><code class="language-julia hljs">using DataSci4Chem

fh2(x) = fit.param[1] .* x .+ fit.param[2]

res = fh2(X) - Y

scatter(X,res,xlabel =&quot;X values&quot;, ylabel = &quot;residuals&quot;, label =false)
plot!([0,105],[0,0],label=false)</code></pre><img src="a4eb7cab.svg" alt="Example block output"/><p>As you can see from the above figure, there is not a clear trend in the distribution of the residuals, indicating the our model is able to capture the underlying trend in our data. In case of observed trends, we can deduce that our objective function is not well suited to describe such trends. </p><h3 id="RMSE"><a class="docs-heading-anchor" href="#RMSE">RMSE</a><a id="RMSE-1"></a><a class="docs-heading-anchor-permalink" href="#RMSE" title="Permalink"></a></h3><p><a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> provides a metrics of the magnitude of the error in the model predictions. As you can deduce from the name, it is a combined measure of residuals. </p><p class="math-container">\[
RMSE = \sqrt{mean(Y - f(X))}
\]</p><p>Smaller RMSE values indicate that the model is able to explain higher levels of variance in the data. There is not a clear cut off for RMSE for accepting or rejecting a model. It is mainly used for comparing two or more models against each other.</p><h3 id="R2"><a class="docs-heading-anchor" href="#R2"><span>$R^2$</span></a><a id="R2-1"></a><a class="docs-heading-anchor-permalink" href="#R2" title="Permalink"></a></h3><p><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><span>$R^2$</span></a> similarly to the other measures is an indicator for the goodness of fit. <span>$R^2$</span> in practice is showing how much better your model is in describing your data than the average value. Mathematically the <span>$R^2$</span> is calculated as below</p><p class="math-container">\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - f(x_i))^2}{\sum_{i=1}^{n} (y_i - \bar y)^2}
\]</p><p><span>$R^2$</span> can range between zero, no trend, to one, perfect correlation. The larger <span>$R^2$</span> values show that the fitted model a lot better than the <em><span>$\bar Y$</span></em> in describing our data. For example for our model the <span>$R^2$</span> can be calculated as follows.</p><pre><code class="language-julia hljs">using DataSci4Chem


res_t = fh2(X) .- mean(Y)

R2 = 1 - sum(res.^2)/sum(res_t.^2)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.9658492360404674</code></pre><p>Usually, <span>$R^2$</span> values above 0.6 indicate that the model is able to explain a large portion of variance in the data. </p><div class="admonition is-success"><header class="admonition-header">Tip</header><div class="admonition-body"><p>For complex and multivariate systems, <span>$R^2$</span> values close to one may indicate an issue of <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p></div></div><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Another metrics that can be used for the assessment of your fit quality is the confidence interval (CI) of your model. The CI of your model can be estimated based on the residuals and the t-distribution. There are also numerical approaches such as <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrapping</a> and <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">leave one out</a> to do these calculations. </p></div></div><h2 id="Additional-resources"><a class="docs-heading-anchor" href="#Additional-resources">Additional resources</a><a id="Additional-resources-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-resources" title="Permalink"></a></h2><p>For additional resources, please take a look at this MIT open course ware <a href="https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/resources/lecture-16-projection-matrices-and-least-squares/">here:</a>. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Integration/">« Numerical integration</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.0.0 on <span class="colophon-date" title="Sunday 17 September 2023 18:41">Sunday 17 September 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
